---
title: 'The evolution of Al reward signals'
date: 2025-10-24
permalink: /posts/2012/08/blog-post-1/
tags:
  - RLVR
  - AI
  - Reward Signals
---

# The evolution of Al reward signals

![Generated by Gemini](/assets/images/reward-signals-1.jpg)
*Generated by Gemini*

Over time, we have developed new methods to help AI models understand things. We achieve this by providing reward signals, which teach AI models to favor certain behaviors or outcomes and avoid others.  
However, the approach to assign rewards has evolved from binary signals to fuzzier and more subjective ones. But the goal has been to align AI model outputs closer to human reasoning and intent.  
This article explores the transition from early binary rewards to subjective, rubric-based, and verifiable signals. This progression has enabled AI models to reason more effectively and make better decisions.

## 6 types of AI reward signals
Let’s look at the major types of reward signals that have shaped how AI models learn:

### Binary rewards
Binary reward signals provide feedback to AI models using a boolean criterion. It is typically a 1 for success or 0 for failure upon completing a task. Researchers now call this approach a *sparse reward signal* because feedback is given only for rare or specific events like reaching a goal state. This straightforward approach allowed AI to learn basic strategies. But it couldn’t handle tasks requiring nuance or human judgment.  
However, binary rewards are still used in today's world. A common application is *reinforcement learning with verifiable rewards (RLVR)*. RLVR is mainly used for training LLMs on tasks with a verifiable ground truth like math or coding problems.

![Binary logic](/assets/images/reward-signals-2.jpg)
*Binary logic*

For a car’s speed, 39.9 might be labeled slow by an AI agent. A speed of 40.1 may be labeled fast, though it’s nearly the same. This shows that the system needs flexibility to interpret real world situations.

### Fuzzy rewards
Fuzzy logic allows some flexibility rather than strictly categorizing speeds as fast or slow. It represents uncertainty through degrees of membership. This approach enables the system to recognize values like 39.9 or 40.1.  
Essentially, it measures the degree to which a data point belongs to a fuzzy set.

![Fuzzy logic](/assets/images/reward-signals-3.jpg)
*Fuzzy logic*

Fuzzy rewards are a type of AI feedback that reflect partial correctness or graded performance. The AI models receive rewards proportional to its level of performance. For example, in a driving task, an AI model may receive:  
- *0.9 points* for driving slightly over the ideal speed at 40.1 mph.  
- *0.7 points* for driving slightly under at 39.9 mph.  

This approach allows AI models to learn more nuanced behaviors and better handle real world situations.

### Unverifiable rewards
Unverifiable rewards are the heart of how modern LLMs like ChatGPT are trained. For example, if an AI model is asked to generate art. There’s no objective way to judge if the AI model did a good job. It depends on factors like taste and emotion. Two humans might rate the output differently so the reward is inherently unverifiable.  
So, researchers use human feedback or AI feedback to approximate quality. The model generates several responses to a prompt. Humans or a judge model rank them:  
- *Answer A is better than Answer B.*  
- *Answer C is the most creative.*

This creates a preference dataset that does not contain exact right or wrong labels.  
A smaller neural network is trained to predict how much humans would like an answer based on those rankings. This is called a *reward model*, which is trained from human judgments or an LLM evaluator. Then, the main model like ChatGPT uses classic *reinforcement learning from human feedback (RLHF)* to maximize the predicted reward score generated by the learned model.

![Unverifiable rewards for LLM outputs](/assets/images/reward-signals-4.jpg)
*Unverifiable rewards for LLM outputs*

The graph illustrates the distribution of subjective reward scores. You can see that there is no single correct score but the reward follows a distribution. The width of this distribution corresponds to the variance or standard deviation. This indicates the degree of inter-judge agreement for that particular AI model output.  
For instance, output *z* has been rated very similarly by different human judges or an LLM evaluator. Their scores were all clustered closely around the mean or the center of the curve. This makes output *z* a clear and consistent preference signal, which could be either positive or negative. There is low uncertainty about its true reward value. This makes it a high quality data point for training the LLM.  
The average rewards represent the mean of the combined rewards assigned by human annotators. They indicate the quality of the AI model output based on human feedback data.  
The Reward Model evaluates all outputs and mathematically computes a reward distribution that optimally balances a high mean with low variance. For example, for output *z*:  
- *Centered high:* This means the expected reward for this output is high.  
- *Low variance:* This defines the certainty of the high score.  

In this case, output *z* was a rather easy choice.

### Rubric-based rewards
A rubric-based reward applies like a teacher’s grading guide. A rubric is essentially a set of evaluation criteria. Rubric-based reward systems are commonly used in structured RLHF or *reinforcement learning from AI feedback (RLAIF)*.  
The process begins with an AI model generating output. Next, a human or AI evaluator scores the output using a rubric that may include criteria such as *accuracy, helpfulness, clarity,* etc.  
Then, a reward signal can be computed using a weighted combination as follows:  
**R = 0.4 × accuracy + 0.3 × clarity + 0.3 × helpfulness**  
The base model learns to maximize the composite reward and gradually aligns its behavior with all rubric dimensions.  
In 2025, many labs, including Anthropic and OpenAI, are using LLMs to apply rubrics automatically.

![Rubric rewards comparison](/assets/images/reward-signals-5.jpg)
*Rubric rewards comparison*

For example, GPT-4 can read a rubric like, *‘Rate this answer on accuracy, clarity, and helpfulness from 1 to 5 each.’* Then it might produce scores for each parameter like this:  
- Accuracy: 5  
- Clarity: 3  
- Helpfulness: 4  

It will compute the final reward as a weighted average using the following formula:  
**R = 0.4 × 5 + 0.3 × 3 + 0.3 × 4 = 3.9**  

This is called *rubric-based evaluation.* When a composite reward like 3.9 is produced, it’s just a numerical summary of several rubric criteria. So, the 4.2 is the final reward signal given to the model during training. Although the model was clear and helpful, it was not fully accurate, which is why it lost points.  
In RL, the model doesn’t directly see the breakdown like *accuracy = 3.* It just gets the total reward 3.9. However, because thousands of examples are scored using the same rubric, the model statistically associates certain behaviors with higher or lower rewards. For example:  
- When the answers contain complex or heavy wording, the overall reward is lower.  
- When explained clearly, the total reward is greater.  

Over many updates, it learns the latent pattern that simpler wording produces a better reward.  
However, some newer systems like LLM evaluators do keep rubric feedback visible during training.  
They don’t just achieve a 3.9, they earn a structured reward like:  
`{ "Accuracy": 5, "Clarity": 3, "Helpfulness": 4 }`

Then, a downstream model like a policy optimizer can learn separate reward heads, with each modeling one dimension. This provides even finer control because you can directly penalize less clarity. Additionally, you can train different policies that emphasize different rubric weights.

### Verifiable rewards
Verifiable rewards are a revival of the old idea in which we prove the answer is right. Researchers use verifiable rewards when a ground truth exists. A clear and checkable answer must exist for the answer to be mathematically or logically validated. Verifiable rewards rely on *deterministic correctness* unlike unverifiable or rubric-based rewards, which depend on human judgment.  
For example, if an AI model is asked to solve a math problem, we can directly compute whether it’s right or wrong. This makes the reward signal fully grounded in truth rather than opinion.  
In verifiable reward systems, correctness is evaluated automatically:  
- If the answer is correct, the model receives a high reward of *1.*  
- If the answer is incorrect, the reward is low or *0.*  

Researchers use verifiable rewards with *RLVR* for tasks that have clear evaluation criteria.  
RLVR is increasingly used to train specialized LLMs. It is particularly useful in domains like math, prompt optimization, and scientific reasoning, where symbolic or numerical verification is possible.

![Proof-based correctness](/assets/images/reward-signals-6.jpg)
*Proof-based correctness*

However, verifiable rewards differ from traditional binary rewards in one key aspect. Binary rewards indicate success or failure based on achieving a goal state. For example, a robot reaching a target. Verifiable rewards, on the other hand, use logical or computational proofs to verify the correctness of the answer itself.  
In other words, binary rewards are *structural,* like *'Did you reach the goal?'* And verifiable rewards are *epistemic,* like *'Can we prove you’re correct?'* They are both discrete signals, but verifiable rewards are grounded in objective reasoning and formal validation, not just event completion.  
Verifiable rewards are effective only for problems with objective evaluation. For open-ended creative or linguistic tasks, unverifiable or rubric-based rewards are required.

### Process rewards
Process reward is a relatively recent concept that focuses on how AI models arrive at an answer. These rewards evaluate the reasoning trajectory, intermediate steps, and the chain of thought leading to the conclusion.  
In many reasoning tasks, an answer can be incorrect for the right reasons or correct for the wrong reasons. Process rewards help differentiate between the two.  
For example, in math or multi-step problem solving, the model might do the following sequence of steps:  
- Correctly interprets the problem.  
- Apply the correct formula.  
- Makes a small arithmetic mistake at the end.  

This would score as *0* with verifiable or binary rewards. But with process rewards, it can still earn partial credit because the reasoning structure is sound.

![Rewarding the reasoning path](/assets/images/reward-signals-7.jpg)
*Rewarding the reasoning path*

Process reward models are usually implemented using *stepwise evaluation.* At each stage, the model’s intermediate outputs or hidden reasoning tokens are scored by a verifier or another model trained to judge reasoning quality.  
This approach is used in experiments like *reinforcement learning from process feedback (RLPF)* and *Process Supervision.* For example, OpenAI’s process supervision experiments evaluate each reasoning step using a small verifier model that assesses whether a step logically follows from the preceding one.  
The system then aggregates the total process reward from all step evaluations:  
**R = (Σ step_rewards) / total_steps**  
This approach makes the learning signal denser and more informative than outcome-based feedback alone. 

### Final remarks
The evolution of AI reward signals has progressed from explicit to implicit functions. In earlier systems, rewards were explicitly defined by clear success criteria. However, modern approaches infer rewards implicitly from patterns of human preferences, reasoning quality, or verifiable logic. This advancement also addresses the vulnerability of earlier methods to reward hacking.  
Today, two new directions stand out which are rubric-based rewards, where small sets of rules guide alignment, and a revival of verifiable correctness for math and coding through *RLVR.*  
*Process rewards* are also emerging as a means to score intermediate reasoning steps, providing a middle ground.
